# -*- coding: utf-8 -*-
"""HEART DISEASE PREDICTION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W5Bjil3czECu7Mw4LS9Rkq8Gc6Wa3g2I
"""

# Basic Libraries
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="whitegrid")

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Warnings
import warnings
warnings.filterwarnings("ignore")

# Load Dataset
df = pd.read_csv("/content/heart.csv")

# Preview Dataset
df.head()
# Shape of the dataset
print("Shape of dataset:", df.shape)

# Data types and missing values
print("\nInfo:")
print(df.info())

# Summary statistics
print("\nSummary statistics:")
df.describe()
# Target column distribution
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='HeartDisease', palette='Set2')
plt.title("Heart Disease Distribution")
plt.xticks([0, 1], ["No Disease", "Disease"])
plt.ylabel("Count")
plt.show()

# Percentage distribution
percent = df['HeartDisease'].value_counts(normalize=True) * 100
print(percent)
# Encode categorical columns temporarily for correlation
df_encoded = df.copy()
df_encoded['Sex'] = df_encoded['Sex'].map({'M': 1, 'F': 0})
df_encoded['ChestPainType'] = df_encoded['ChestPainType'].map({'ATA': 0, 'NAP': 1, 'ASY': 2, 'TA': 3})
df_encoded['RestingECG'] = df_encoded['RestingECG'].map({'Normal': 0, 'ST': 1, 'LVH': 2})
df_encoded['ExerciseAngina'] = df_encoded['ExerciseAngina'].map({'N': 0, 'Y': 1})
df_encoded['ST_Slope'] = df_encoded['ST_Slope'].map({'Up': 0, 'Flat': 1, 'Down': 2})

# Correlation heatmap
plt.figure(figsize=(12,8))
sns.heatmap(df_encoded.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()
# Copy the original dataframe
data = df.copy()

# Encode categorical columns manually
data['Sex'] = data['Sex'].map({'M': 1, 'F': 0})
data['ChestPainType'] = data['ChestPainType'].map({'ATA': 0, 'NAP': 1, 'ASY': 2, 'TA': 3})
data['RestingECG'] = data['RestingECG'].map({'Normal': 0, 'ST': 1, 'LVH': 2})
data['ExerciseAngina'] = data['ExerciseAngina'].map({'N': 0, 'Y': 1})
data['ST_Slope'] = data['ST_Slope'].map({'Up': 0, 'Flat': 1, 'Down': 2})

# Check the result
data.head()
# Count how many 0s exist
print("RestingBP zero count:", (data['RestingBP'] == 0).sum())
print("Cholesterol zero count:", (data['Cholesterol'] == 0).sum())

# Replace 0s with median (more robust than mean)
data['RestingBP'] = data['RestingBP'].replace(0, data['RestingBP'].median())
data['Cholesterol'] = data['Cholesterol'].replace(0, data['Cholesterol'].median())

# Confirm replacement
print("\nAfter replacement:")
print("RestingBP zero count:", (data['RestingBP'] == 0).sum())
print("Cholesterol zero count:", (data['Cholesterol'] == 0).sum())
# Split features and target
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']

# Train-test split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Show shapes
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)
# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train_scaled, y_train)

# Predictions
y_pred_lr = log_reg.predict(X_test_scaled)
y_prob_lr = log_reg.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob_lr)
plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {roc_auc_score(y_test, y_prob_lr):.2f})")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.show()
# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_scaled, y_train)

# Predictions
y_pred_rf = rf.predict(X_test_scaled)
y_prob_rf = rf.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens')
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob_rf)
plt.plot(fpr, tpr, label=f"Random Forest (AUC = {roc_auc_score(y_test, y_prob_rf):.2f})")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Random Forest")
plt.legend()
plt.show()
# XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X_train_scaled, y_train)

# Predictions
y_pred_xgb = xgb.predict(X_test_scaled)
y_prob_xgb = xgb.predict_proba(X_test_scaled)[:, 1]

# Evaluation
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt='d', cmap='Oranges')
plt.title("Confusion Matrix - XGBoost")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_prob_xgb)
plt.plot(fpr, tpr, label=f"XGBoost (AUC = {roc_auc_score(y_test, y_prob_xgb):.2f})")
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - XGBoost")
plt.legend()
plt.show()
# Feature Importance - Random Forest
importances = rf.feature_importances_
features = X.columns

# Create a DataFrame for easier plotting
feat_imp = pd.DataFrame({'Feature': features, 'Importance': importances})
feat_imp = feat_imp.sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data=feat_imp, palette='viridis')
plt.title("Feature Importance - Random Forest")
plt.show()
import shap

# SHAP requires TreeExplainer for tree-based models like RandomForest
explainer = shap.TreeExplainer(rf)
shap_values = explainer.shap_values(X_test_scaled)

# SHAP summary plot
shap.initjs()
shap.summary_plot(shap_values[1], X_test_scaled, plot_type="bar", feature_names=X_test.columns)
# Visualize explanation for a single prediction
i = 0  # index of sample from test set
shap.force_plot(explainer.expected_value[1], shap_values[1][i], X_test_scaled[i].reshape(1, -1), feature_names=X_test.columns)